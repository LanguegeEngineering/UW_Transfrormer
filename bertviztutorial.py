# -*- coding: utf-8 -*-
"""BertvizTutorial.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HTD8sa483gkXW8VnFMXods_9Q86Fyqb6

# BertViz Interactive Tutorial
"""

!pip install bertviz

# Load model and retrieve attention weights

from bertviz import head_view, model_view
from transformers import AutoModel, AutoTokenizer

model_version = 'allegro/herbert-base-cased'
model = AutoModel.from_pretrained(model_version, output_attentions=True)
tokenizer = AutoTokenizer.from_pretrained(model_version)
sentence_a = "Jestem bardzo zmÄ™czony w tym miesiÄ…cu"
sentence_b = "ChcÄ™ siÄ™Â napiÄ‡ wody i Å¼eby w niej byÅ‚a miÄ™ta."
inputs = tokenizer.encode_plus(sentence_a, sentence_b, return_tensors='pt')
input_ids = inputs['input_ids']
token_type_ids = inputs['token_type_ids']
attention = model(input_ids, token_type_ids=token_type_ids)[-1]
sentence_b_start = token_type_ids[0].tolist().index(1)
input_id_list = input_ids[0].tolist() # Batch index 0
tokens = tokenizer.convert_ids_to_tokens(input_id_list)

"""# Head View
<b>The head view visualizes attention in one or more heads from a single Transformer layer.</b> Each line shows the attention from one token (left) to another (right). Line weight reflects the attention value (ranges from 0 to 1), while line color identifies the attention head. When multiple heads are selected (indicated by the colored tiles at the top), the corresponding  visualizations are overlaid onto one another.  For a more detailed explanation of attention in Transformer models, please refer to the [blog](https://towardsdatascience.com/deconstructing-bert-part-2-visualizing-the-inner-workings-of-attention-60a16d86b5c1).

## Usage
ðŸ‘‰ **Hover** over any **token** on the left/right side of the visualization to filter attention from/to that token. <br/>
ðŸ‘‰ **Double-click** on any of the **colored tiles** at the top to filter to the corresponding attention head.<br/>
ðŸ‘‰ **Single-click** on any of the **colored tiles** to toggle selection of the corresponding attention head. <br/>
ðŸ‘‰ **Click** on the **Layer** drop-down to change the model layer (zero-indexed).

"""



head_view(attention, tokens, sentence_b_start)

"""# Model View
<b>The model view provides a birds-eye view of attention throughout the entire model</b>. Each cell shows the attention weights for a particular head, indexed by layer (row) and head (column).  The lines in each cell represent the attention from one token (left) to another (right), with line weight proportional to the attention value (ranges from 0 to 1).  For a more detailed explanation, please refer to the [blog](https://towardsdatascience.com/deconstructing-bert-part-2-visualizing-the-inner-workings-of-attention-60a16d86b5c1).

## Usage
ðŸ‘‰ **Click** on any **cell** for a detailed view of attention for the associated attention head (or to unselect that cell). <br/>
ðŸ‘‰ Then **hover** over any **token** on the left side of detail view to filter the attention from that token.
"""

model_view(attention, tokens, sentence_b_start)

"""# Neuron View
<b>The neuron view visualizes the intermediate representations (e.g. query and key vectors) that are used to compute attention.</b> In the collapsed view (initial state), the lines show the attention from each token (left) to every other token (right). In the expanded view, the tool traces the chain of computations that produce these attention weights. For a detailed explanation of the attention mechanism, please refer to the [blog](https://towardsdatascience.com/deconstructing-bert-part-2-visualizing-the-inner-workings-of-attention-60a16d86b5c1).

## Usage
ðŸ‘‰ **Hover** over any of the tokens on the left side of the visualization to filter attention from that token.<br/>
ðŸ‘‰ Then **click** on the **plus** icon that is revealed when hovering. This exposes the query vectors, key vectors, and other intermediate representations used to compute the attention weights. Each color band represents a single neuron value, where color intensity indicates the magnitude and hue the sign (blue=positive, orange=negative).<br/>
ðŸ‘‰ Once in the expanded view, **hover** over any other **token** on the left to see the associated attention computations.<br/>
ðŸ‘‰ **Click** on the **Layer** or **Head** drop-downs to change the model layer or head (zero-indexed).

"""

from bertviz.transformers_neuron_view import BertModel, BertTokenizer
from bertviz.neuron_view import show

model_type = 'bert'
model_version = 'bert-base-uncased'
sentence_a = "I have no idea where is cat."
sentence_b = "I have seen it a long time ago."

model = BertModel.from_pretrained(model_version, output_attentions=True)
tokenizer = BertTokenizer.from_pretrained(model_version, do_lower_case=True)
show(model, model_type, tokenizer, sentence_a, sentence_b, layer=5, head=5)